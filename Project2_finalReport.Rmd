---
title: "Logistic Regression Analysis of Banknotes Authenticity"
author: "C M Nafi and Sneha Verma"
date: "1/27/2021"
output: word_document
---

## Abstract

DO AT THE END OF THE PROJECT

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

NOTE: Before running the file, make sure that the location of the data corresponds with the location on your device.

This dataset (obtained from [Kaggle](https://www.kaggle.com/ritesaluja/bank-note-authentication-uci-data)) was generated from genuine and forged banknote images.An industrial camera was used for print inspection and the Wavelet Transform tool were used to extract features from the images.

This research project will focus on the central question:

**How well can the image features predict if the banknote is non-counterfeit?**

The dataset has 5 features and 1372 observations. The features are as follows:

1) **variance**: is a continuous value which finds how each pixel varies from the neighboring pixels and classifies them into different regions. If the pixel values are close to the mean pixel value than the variance is low and if they are not close to the mean pixel value than it is a high variance.

2) **skewness**: is a continuous value measuring the lack of symmetry. A symmetric data would have a value of 0. Negative values indicate left-skewness and positive values indicate right-skewness.

3) **kurtosis**: a continuous values measuring whether the data is heavy-tailed or light-tailed relative to a normal distribution. Datasets with high kurtosis have heavy-tails or outliers and those with low kurtosis have light tails or lack of outliers.

4) **entropy**: is a continuous value representing a quantity used to describe the amount of information which must be coded for, by a compression algorithm. A higher entropy value implies that there is a higher amount of information in the image.

5) **class**: is the target, binary variable where a value of 0 represents a geunine or authentic bank note and a value of 1 represents a fake note.

## Import Dataset

```{r}
banknote = read.csv('Original_Banknote_Authentication.csv')
head(banknote)

# Let us rename the variable curtosis to the more commonly used spelling
names(banknote)[names(banknote) == 'curtosis'] = 'kurtosis'
names(banknote)[names(banknote) == 'class'] = 'notAuthentic'
```


## Data Characteristics

Looking at this dataset, it can be observed that there are five variables (one of them is the target variable: notAuthentic). 

### Scatterplot Matrix

Let us create a scatterplot matrix coloured by the outcome variable:

```{r}
pairs(banknote, col=banknote$notAuthentic+1)
```

The scatterplot matrix shows that there might be a relationship between some of the predictor variables and the response variable. It appears that for lower values of variance in the image, the banknote is likely to be fake and images with higher variance values were of genuine bank notes. Further, it appears that genuine bank notes have higher skewness; however, there does not appear to be a high level of correlation because fake bank notes also have relatively high bank skewness. Similarly, genuine bank notes have lower values of kurtosis as compared to fake notes; however, fake notes have kurtosis values of the entire range, this may make it harder to differentiate between genuine and fake banknotes. Lastly, it appears that entropy is not a good predictor of authenticity because images of fake and genuine banknotes have entropy values of the entire range of possible values indicating a weak relationship between teh two variables.

### Correlation Matrix

```{r}
#install.packages('reshape2')
#install.packages('ggplot2')

banknoteCorr <- round(cor(banknote),2)
head(banknoteCorr)

library(reshape2)
melted_banknote <- melt(banknoteCorr)
head(melted_banknote)

library(ggplot2)
ggplot(data = melted_banknote, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()
```

This matrix shows the correlation matrix between the variables in the data set. Lighter boxes indicate higher correlation values. 

It appears that there is a high correlation between the authenticity of banknotes and the kurtosis. There also appears to be a relatively higher correlation between authenticity and entropy compared to the correlation between skewness and variance with authenticity. This is interesting because the scatterplot matrix between authenticity and entropy does not show a strong relationship.

### Plots of Individual Variables

#### variance

```{r}
summary(banknote$variance)
```

The minimum value of variance is -7 and the maximum value is 6.8 with a median of 0.496 and a mean of 0.434. 

```{r fig.height=5, fig.width=8.5}
par(mfrow=c(1,2))
hist(banknote$variance, xlab = "Distribution of image variance")
boxplot(banknote$variance, xlab = "Distribution of image variance")
```

The histogram of the variance shows mild left-skewness; however, since it the skewness is only slight, no transformation will be conducted. Further, the boxplot shows that there are no outliers in this variable. 

#### skewness

```{r}
summary(banknote$skewness)
```

The minimum value of skewness is -13.77 and the maximum value is 12.95 with a median of 2.32 and a mean of 1.92. 

```{r fig.height=5, fig.width=8.5}
par(mfrow=c(1,2))
hist(banknote$skewness, xlab = "Distribution of image skewness")
boxplot(banknote$skewness, xlab = "Distribution of image skewness")
```

The histogram of the skewness shows slight or moderate left-skewness; however, since it the skewness is only slight, no transformation will be conducted.
Further, the boxplot shows that there are no outliers in this variable. The boxplot also shows the left-skewness with a long lower tail.

#### kurtosis

```{r}
summary(banknote$kurtosis)
```

The minimum value of variance is -5.29 and the maximum value is 17.93 with a median of 0.617 and a mean of 1.398. 


```{r fig.height=5, fig.width=8.5}
par(mfrow=c(1,2))
hist(banknote$kurtosis, xlab = "Distribution of image kurtosis")
boxplot(banknote$kurtosis, xlab = "Distribution of image kurtosis")
```

The histogram of the variance shows moderate right-skewness. Further, the boxplot displays this skewness with a longer upper tail and also displays outliers at the upper end of the distribution. However, since there is moderate skewness, we will not transform the variable at this point.

#### entropy

```{r}
summary(banknote$entropy)
```

The minimum value of variance is -8.55 and the maximum value is 2.45 with a median of -0.59 and a mean of -1.19. 


```{r fig.height=5, fig.width=8.5}
par(mfrow=c(1,2))
hist(banknote$entropy, xlab = "Distribution of image entropy")
boxplot(banknote$entropy, xlab = "Distribution of image entropy")
```

The histogram of the variance shows moderate left-skewness. Further, the boxplot displays this skewness with a long lower tail and also displays outliers at the lower end of the distribution. However, since there is only moderate skewness, we will not apply a transformation at the point.


## First-Order Logistic Regression Model

Let us now fit a logistic regression model that includes all of the variables.

```{r}
fit1 = glm(notAuthentic ~ variance + skewness + kurtosis + entropy, data = banknote, family = binomial)
summary(fit1)
```

While this fit products a warning message, it does converge on parameter estimates and p-values. From the resultant summary, we can see that all variables are significant at the 0.001 significance level, except for entropy which is significant at the 0.1 level. The slopes are given in the logit form, let us convert it to the odds ratio.

```{r}
exp(fit1$coefficients)
exp(confint(fit1))
```

The odds of the note being fake is 0.00039 times higher for each unit of variance with a 95% confidence interval of 0.0000059 to 0.00621 higher odds of the note being fake per variance unit.

The odds of the note being fake is 0.0151 times higher for each unit of skewness with a 95% confidence interval of 0.00171 to 0.0641 higher odds of the note being fake per skewness unit.

The odds of the note being fake is 0.00505 times higher for each unit of kurtosis with a 95% confidence interval of 0.000306 to 0.0320 higher odds of the note being fake per kurtosis unit.

The odds of the note being fake is 0.546 times higher for each unit of entropy with a 95% confidence interval of 0.273 to 1.04 higher odds of the note being fake per entropy unit.

### Model Diagnostics

1) Jittered response v/s predicted values

```{r fig.height=3.5, fig.width=7.5}
par(mfrow=c(1,2))
logit.fit1 = predict(fit1)
plot (jitter (notAuthentic, 0.2) ~ logit.fit1, data=banknote)

pihat.fit1 = predict(fit1, type='response')
pihat.ord = order(pihat.fit1)
lines (logit.fit1[pihat.ord], pihat.fit1[pihat.ord])
lines (lowess (logit.fit1[pihat.ord], pihat.fit1[pihat.ord]), col='red', lty=2)

```

From the "Jittered response v/s predicted values"  plot, we can see the probability of predicted logit. We can see that the probability is close to 1 when logit values are greater than 0. We are going to check the residual v/s Fitted values next to check the linearity. 

2) Residuals v/s Fitted Values

```{r}
plot(fit1, which = 1)
```

The trendline in the residual v/s fitted plot shows little to no deviation from the horizontal zero line showing that the distribution of the residuals is not concerning; it shows clear linearity. Further, there do appear to be a few points with relatively higher outliers; however, they are not greater than 3 so it is not concerning.

3) Leverage

```{r}
plot(fit1, which = 5)
```

In this 'Residuals vs Leverage plot', we are trying to check unusual points in this dataset. 
We can see that there are some possible large residuals at datapoint 919. However, it's close to 3 so we are not going to react to that. 

We can also check the Cook's distance to see the amount of influence a particular data point has. 
Our moderate Cook's D cutoff point is ( 0.00728 ) and our Unusual Cook's D cutoff point is (0.01093). There are several points that are showing large cook's distance.



(2(k+1))/n > somewhat high
(3(k+1))/n > extremely high

4) Multicollinearity

```{r}
car::vif(fit1)
```

We can see that some predictors are highly correlated to each other. 

## Model Selection

### Stepwise Regression

Let us now apply stepwise regression to the first-order model to see which variables should be kept in the model.

```{r}
fit1aic = step(fit1, direction = 'both')
summary(fit1aic)
```

According to this result, there are no steps that can be taken to decrease the value of AIC.

Let us now try stepwise regression with the BIC criterion:


```{r}
fit1bic = step(fit1, direction = "both", k = log(fit1$rank + fit1$df.residual))

summary(fit1bic)
```

The result in BIC stepwise regression removed entropy from the model.  

We can also see that the residual deviance is slightly better in AIC ( 49.891) compare to BIC (53.299) which is not too bad. As BIC criterion removed entropy from the model, which is not very significant. We are going to use the BIC model.

/// Tried interaction here:

Now we are going to check interaction effect 


```{r}
fit_int1 = glm (notAuthentic ~ variance + skewness + kurtosis + entropy + 
                    entropy:(variance + skewness + kurtosis), data=banknote, family=binomial)
summary(fit_int1)
```

Stepwise here:

```{r}
fit_int1bic = step(fit_int1, direction = "both", k = log(fit1$rank + fit1$df.residual))

summary(fit_int1bic)

```

BIC removes every interaction


How about AIC:
```{r}

fit_int1_aic = step(fit_int1, direction = 'both')
summary(fit_int1_aic)

```

fit1bic = step(fit1, direction = "both", k = log(fit1$rank + fit1$df.residual))

summary(fit1bic)

